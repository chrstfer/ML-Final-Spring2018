# -*- org-confirm-babel-evaluate: nil; -*-
#+AUTHOR: Matthew Sears
#+TITLE: Artificial Neural Nets 
#+HTML_HEAD: <link href="http://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
#+PROPERTY: header-args :session ANNsb


* Setup
** R Setup  
*** Libraries and Functions
  #+BEGIN_SRC R :results none :export source
    library(sigmoid)
    getLabels <- function(lbldb, nget=0) {
	magic  <- readBin(lbldb, what="integer", n=1, endian="big", size=4)
	if(magic != 2049)
	    return(NULL)
	n.lbls    <- readBin(lbldb, what="integer", n=1,    endian="big", size=4)
	if(nget==0)
	    nget=n.lbls

	labels <- readBin(lbldb, what="integer", n=nget, endian="big",  size=1)

	close(lbldb)
	return(labels)
    }

    getImages <- function(imgdb, nget=0, progress=FALSE) {
	magic  <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	## if(magic != 2049)
	##     return(NULL)

	n.imgs <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	if(nget==0)
	    nget <- n.imgs # trunc(sqrt(n.imgs))

	n.rows <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	n.cols <- readBin(imgdb, what="integer", n=1, endian="big", size=4)

	print(gettextf("Getting %d %dx%d Images", nget, n.rows, n.cols))

	images <- c()
	for(i in 1:nget) {
	    .img   <- matrix(readBin(imgdb, what="integer", n=n.rows*n.cols, endian="big", size=1), nrow=n.rows, ncol=n.cols)
	    images <-  c(images, list(.img))
	    if(progress && i %% trunc(sqrt(nget)) == 0) 
		print(gettextf("%2.2f%%", round((100*i)/nget, digits=2)))
	}
	close(imgdb)
	return(images)
    }
  #+END_SRC
** Data Setup
*** Data (import)
#+BEGIN_SRC R :results output graphics :file imgs/setup/ex1.png
  ## Works
  tr.labels <- as.vector(getLabels(gzfile("datasets/training/labels", "rb"), nget=256))
  tr.images <- getImages(gzfile("datasets/training/images", # data's filename
				"rb"), # read it as binary
			 ## Get 256 of the entries
			 nget=256, progress=TRUE)
  tr.im.matrix <- do.call("rbind", # create rows out of the input data
			  lapply(tr.images, as.vector)) # transform each image
							# matrix into a vector

  tr.df <- cbind(as.factor(tr.labels), tr.im.matrix) # now create a data frame

  oldpar <- par(mar=rep(0,4))
  image(tr.images[[8]], useRaster=TRUE, col=seq(2^8)) 
  par(oldpar)
#+END_SRC

#+RESULTS:
[[file:imgs/setup/ex1.png]]

- Label Frequency ::
#+BEGIN_SRC R :results table drawer :colnames yes :exports results
table(Labels=tr.df[,ncol(tr.df)])
#+END_SRC

#+RESULTS:
:RESULTS:
| Labels | Freq |
|--------+------|
|      0 |  256 |
:END:

*** Data (Links)
  |---------------------+----------+-------------------------------------------------------------|
  | ID                  | size (b) | Link                                                        |
  |---------------------+----------+-------------------------------------------------------------|
  | training set images |  9912422 | http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz |
  | training set labels |    28881 | http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz |
  | test set images     |  1648877 | http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  |
  | test set labels     |     4542 | http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz  |
  |---------------------+----------+-------------------------------------------------------------|



* Simple

#+BEGIN_SRC R :exports both :results output
  #rm(list=ls())
  set.seed(1)

  activate <- function(node)
      return(matrix(1/(1+exp(-node))))
  sigprime <- function(node)
      return(matrix(activate(node)*(1 - activate(node))))

  model <- list()
  truth <- 1
  learningrate <- 0.25
  errs <- c()

  .lengths <- c(2,2,1) # I,H,O

  model$input <- c(1,0)

  .nlayers <- length(.lengths)

  model$nodes <- mapply(matrix,
			data=1,
			ncol=1,
			nrow=.lengths)

  model$weights <-  lapply(1:(.nlayers-1),
			   function(k) {
			       matrix(rnorm(.lengths[k+1]*.lengths[k]),
				      nrow=.lengths[k+1],
				      ncol=.lengths[k])
			   })

  .b <- numeric()
  .b <- lapply(.lengths[-1], rnorm)
  model$biases <- mapply(matrix,
			 data=.b,
			 ncol=1,
			 nrow=.lengths[-1])

  for(i in 1:250) {

			      # Feed Forward

      model$nodes[[1]] <- matrix(model$input)

      model$active[[1]] <- activate(model$nodes[[1]])

      model$nodes[[2]] <- model$weights[[1]]%*%model$active[[1]]
			  +model$biases[[1]]

      model$active[[2]] <- activate(model$nodes[[2]])

      model$nodes[[3]] <- model$weights[[2]]%*%model$active[[1]]
			  +model$biases[[2]]

      model$active[[3]] <- activate(model$nodes[[3]])

      for(i in 1:3) {
	  model$nodes[[2]] <- model$weights[[1]]%*%model$active[[1]]
	  +model$biases[[1]]
      }
    
    
			      # Backprop

      #cat("PREDICTION:", model$active[[3]], "\n")
      #cat("ERROR:", model$active[[3]]-truth, "\n")
      errs[i] <- model$active[[3]] - truth

      del <- list()

      del[[2]] <- (model$active[[3]] - truth) * sigprime(model$nodes[[3]])

      del[[1]] <- (t(model$weights[[2]])%*%del[[2]]) * sigprime(model$nodes[[2]])


			      # Update

      w2 <- model$weights[[2]] - learningrate*( del[[2]]%*%t(model$active[[2]]) )

      w1 <- model$weights[[1]] - learningrate*( del[[1]]%*%t(model$active[[1]]) )

      model$weights[[2]] <- w2

      model$weights[[1]] <- w1


      b2 <- model$biases[[2]] - learningrate*del[[2]]

      b1 <- model$biases[[1]] - learningrate*del[[1]]

      model$biases[[2]] <- b2

      model$biases[[1]] <- b1   
  }
#+END_SRC

#+RESULTS:

** Results
#+BEGIN_SRC R :results graphics :exports output :file plot.png
plot(abs(errs))
#+END_SRC

#+RESULTS:
[[file:plot.png]]

** Notes
   - Why separate active vs nodes? why not `model$nodes <- activate(the multiplication)`?
   - 


* MNIST?

Using the first training example.
#+BEGIN_SRC R
  ## Keeping everything else the same as toy example above, except for
  ## this stuff right here
  normalize <- function(x){return(x/sum(x))}

  errs <- list()
  model <- list()
  model$input <- as.vector(tr.images[[1]])
  .lengths <- c(784, 4, 10)
  truth <- matrix(c(0,0,0,0,0,1,0,0,0,0), ncol=1) # 5
  ## ~~

  .nlayers <- length(.lengths)

  model$nodes <- mapply(matrix,
                         data=1,
                         ncol=1,
                         nrow=.lengths)

  model$weights <-  lapply(1:(.nlayers-1),
                            function(k) {
                                matrix(rnorm(.lengths[k+1]*.lengths[k]),
                                       nrow=.lengths[k+1],
                                       ncol=.lengths[k])
                            })

  .b <- numeric()
  .b <- lapply(.lengths[-1], rnorm)
  model$biases <- mapply(matrix,
                         data=.b,
                         ncol=1,
                         nrow=.lengths[-1])


  for(i in 1:250) {
       
                                          # Feed Forward
      

      model$nodes[[1]] <- matrix(model$input)

      model$active[[1]] <- activate(model$nodes[[1]])

      model$nodes[[2]] <- model$weights[[1]]%*%model$active[[1]] +
          model$biases[[1]]

      model$active[[2]] <- activate(model$nodes[[2]])

      model$nodes[[3]] <- model$weights[[2]]%*%model$active[[2]] +
          model$biases[[2]]

      model$active[[3]] <- activate(model$nodes[[3]])

       
                                          # Backprop
      #model$active[[3]] <- normalize(model$active[[3]]) # probability vector
       
      #cat("PREDICTION:", model$active[[3]], "\n")
      #cat("ERROR:", model$active[[3]]-truth, "\n")
      errs[[i]] <- model$active[[3]] - truth

      del <- list()

      del[[2]] <- (model$active[[3]] - truth) * sigprime(model$nodes[[3]])
       
      del[[1]] <- (t(model$weights[[2]])%*%del[[2]]) * sigprime(model$nodes[[2]])
       

                                          # Update
      
       
      w2 <- model$weights[[2]] - learningrate *
          ( del[[2]]%*%t(model$active[[2]]) )
       
      w1 <- model$weights[[1]] - learningrate *
          ( del[[1]]%*%t(model$active[[1]]) )
       
      model$weights[[2]] <- w2
       
      model$weights[[1]] <- w1

      b2 <- model$biases[[2]] - learningrate*del[[2]]
       
      b1 <- model$biases[[1]] - learningrate*del[[1]]
       
      model$biases[[2]] <- b2
       
      model$biases[[1]] <- b1
       
  }
#+END_SRC

#+RESULTS:

Prediction after training
#+BEGIN_SRC R :results both :exports both
which.max(as.vector(model$active[[3]])) -1 #
#+END_SRC

#+RESULTS:
: 5


#+BEGIN_SRC R :results both :exports both
model$active[[3]]
#+END_SRC

#+RESULTS:
| 0.0586737214202425 |
| 0.0576862044600085 |
| 0.0580336343741622 |
| 0.0632149651547858 |
| 0.0527720103665108 |
|  0.939454396075263 |
| 0.0587205980714448 |
|  0.058850624448067 |
| 0.0458369170953963 |
| 0.0530036243472745 |




Error after training
#+BEGIN_SRC R :results both :exports both
sum((model$active[[3]] - truth)^2)
#+END_SRC

#+RESULTS:
: 0.0324069053397839


* Junk that might be useful
#+BEGIN_SRC R
  # ~~~ Junk that might be useful

  ## # save node vectors without activation for backprop
  ## .nodes <- lapply(1:(.nlayers-1),
  ##                   function(k) {
  ##                       model$weights[[k]]%*%model$nodes[[k]]
  ##                       + model$biases[[k]]
  ##                   })

  ## yhat <- activate(model$nodes[[.nlayers]])

  ## ## Backprop
  ## err <- yhat - truth
  ## err

  ## # For every activated output node in err vector, apply element-wise
  ## # multiplication to derivative of activation function of output
  ## # node. This is the "gradient" at the output layer.
  ## nabla <- function(err, layer){
  ##     return(matrix(err*sigprime(.nodes[[layer-1]])))
  ## }

  ## dely <- nabla(err=err,3)
  ## dely


  ## delcdelw2 <- model$nodes[[3]]%*%dely
  ## model$weights[[2]] <- model$weights[[2]] + t(matrix(learningrate*delcdelw2))
  ## model$biases[[2]] <- model$biases[[2]] + learningrate*dely


  ## delw2 <- nabla(err=t(model$weights[[2]])%*%dely, 2)
  ## delw2

  ## delcdelw1 <- model$nodes[[1]]%*%delw2
  ## model$weights[[1]] <- model$weights[[1]] + t(matrix(learningrate*delcdelw1))
  ## model$biases[[1]] <- model$biases[[1]] + learningrate*delw2


  ## model$biases[[2]] <- model$biases[[2]] + delw2 

  ## ???
  ## model$nodes[-1] <- lapply(1:(.nlayers-1),
  ##                           function(k) {
  ##                               activate(model$weights[[k]]%*%model$nodes[[k]]
  ##                                        + model$biases[[k]])
  ##                           })
#+END_SRC


# -*- org-confirm-babel-evaluate: nil; -*-
#+AUTHOR: Matthew Sears
#+TITLE: Artificial Neural Nets 
#+HTML_HEAD: <link href="http://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
#+PROPERTY: header-args :session ANNsb


* Setup
** R Setup  
*** Libraries and Functions
  #+BEGIN_SRC R :results none :export source
    library(sigmoid)
    getLabels <- function(lbldb, nget=0) {
	magic  <- readBin(lbldb, what="integer", n=1, endian="big", size=4)
	if(magic != 2049)
	    return(NULL)
	n.lbls    <- readBin(lbldb, what="integer", n=1,    endian="big", size=4)
	if(nget==0)
	    nget=n.lbls

	labels <- readBin(lbldb, what="integer", n=nget, endian="big",  size=1)

	close(lbldb)
	return(labels)
    }

    getImages <- function(imgdb, nget=0, progress=FALSE) {
	magic  <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	## if(magic != 2049)
	##     return(NULL)

	n.imgs <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	if(nget==0)
	    nget <- n.imgs # trunc(sqrt(n.imgs))

	n.rows <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	n.cols <- readBin(imgdb, what="integer", n=1, endian="big", size=4)

	print(gettextf("Getting %d %dx%d Images", nget, n.rows, n.cols))

	images <- c()
	for(i in 1:nget) {
	    .img   <- matrix(readBin(imgdb, what="integer", n=n.rows*n.cols, endian="big", size=1), nrow=n.rows, ncol=n.cols)
	    images <-  c(images, list(.img))
	    if(progress && i %% trunc(sqrt(nget)) == 0) 
		print(gettextf("%2.2f%%", round((100*i)/nget, digits=2)))
	}
	close(imgdb)
	return(images)
    }
  #+END_SRC
** Data Setup
*** Data (import)
#+BEGIN_SRC R :results output graphics :file imgs/setup/ex1.png
  ## Works
  indata.labels <- as.vector(getLabels(gzfile("datasets/training/labels", "rb"), nget=256))
  indata.images <- getImages(gzfile("datasets/training/images", # data's filename
				    "rb"), # read it as binary
			     ## Get 256 of the entries
			     nget=256, progress=TRUE)
  indata.im.matrix <- do.call("rbind", # create rows out of the input data
			      lapply(tr.images, as.vector)) # transform each image
					  # matrix into a vector

  indata <- cbind(as.factor(indata.labels), indata.im.matrix) # now create a data frame

  ts.idx <- sample(256,64) ## 4:1 tr:ts (for now)

  ts.df <- indata[ts.idx,]
  ts.im <- ts.df[-1,]
  ts.lb <- ts.df[1,]

  tr.df <- indata[-ts.idx,]
  tr.im <- tr.df[,-1]
  tr.lb <- tr.df[,1]

  oldpar <- par(mar=rep(0,4))
  image(tr.images[[8]], useRaster=TRUE, col=seq(2^8)) 
  par(oldpar)
#+END_SRC

#+RESULTS:
[[file:imgs/setup/ex1.png]]

- Label Frequency ::
#+BEGIN_SRC R :results table drawer :colnames yes :exports results
table(Labels=tr.df[,ncol(tr.df)])
#+END_SRC

#+RESULTS:
:RESULTS:
| Labels | Freq |
|--------+------|
|      0 |  256 |
:END:

*** Data (Links)
  |---------------------+----------+-------------------------------------------------------------|
  | ID                  | size (b) | Link                                                        |
  |---------------------+----------+-------------------------------------------------------------|
  | training set images |  9912422 | http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz |
  | training set labels |    28881 | http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz |
  | test set images     |  1648877 | http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  |
  | test set labels     |     4542 | http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz  |
  |---------------------+----------+-------------------------------------------------------------|



* Simple
  
#+BEGIN_SRC R :exports both :results output
  #rm(list=ls())
  set.seed(1)

  activate <- function(node)
      return(matrix(1/(1+exp(-node))))
  sigprime <- function(node)
      return(matrix(activate(node)*(1 - activate(node))))

  truth <- 1
  learningrate <- 0.25
  errs <- c()
  .lengths <- c(2,3,3,1) # I,H,O


  model <- list()
  .nlayers <- length(.lengths)
  model$input <- matrix(c(1,0))
  model$nodes <- mapply(matrix,
			data=lapply(.lengths,rnorm, sd=.37),
			ncol=1,
			nrow=.lengths)
  model$weights <-  lapply(1:(.nlayers-1),
			   function(k) {
			       matrix(rnorm(.lengths[k+1]*.lengths[k]),
				      nrow=.lengths[k+1],
				      ncol=.lengths[k])})
  model$biases <- mapply(matrix,
			 data=lapply(.lengths[-1], rnorm),
			 ncol=1,
			 nrow=.lengths[-1])

  n <- 0
  for(n in 1:250) {
    
			      # Feed Forward
      model$nodes[[1]] <- model$input
      model$active[[1]] <- activate(model$nodes[[1]])

      ## loop through the rest
      for(i in 2:.nlayers) {
	  model$nodes[[i]] <- model$weights[[i-1]] %*% model$active[[i-1]] + model$biases[[i-1]]
	  model$active[[i]] <- activate(model$nodes[[i]])
      }

					  # record error of feed forward
      errs[n] <- model$active[[.nlayers]] - truth

    
					  # Backprop
      del <- list()
      del[[(.nlayers-1)]] <- (model$active[[.nlayers]] - truth) * sigprime(model$nodes[[.nlayers]])
      ## Loop throught the rest
      for(i in seq((.nlayers-1),2,-1)) {
	  print(i)
	  del[[i-1]] <- (t(model$weights[[i]]) %*% del[[i]]) * sigprime(model$nodes[[i]])
      }       

    
					  # Update                                        
      .new.w <- list()
      for(i in length(model$weights):1) {
	  .new.w[[i]] <- model$weights[[i]] - learningrate * del[[i]] %*% t(model$active[[i]])
      }
      model$weights <- .new.w
      .new.b <- list()
      for(i in length(model$weights):1) {
	  .new.b[[i]] <- model$biases[[i]] - learningrate * del[[i]]
      }
      model$biases <- .new.b
  }

    
#+END_SRC

#+RESULTS:

** Results
#+BEGIN_SRC R :results graphics :exports output :file imgs/tests/basic-errplot.png
plot(abs(errs))
#+END_SRC

#+RESULTS:
[[file:imgs/tests/basic-errplot.png]]

** Notes
   - Why separate active vs nodes? why not `model$nodes <- activate(the multiplication)`?
   - 


* MNIST?

** Training
Using the first training example.
#+BEGIN_SRC R :results none
  set.seed(1)

  ## Keeping everything else the same as toy example above, except for
  ## this stuff right here

  normalize <- function(x)
      return(x/sum(x))
  activate <- function(node)
      return(matrix(1/(1+exp(-node))))
  sigprime <- function(node)
      return(matrix(activate(node)*(1 - activate(node))))

  learningrate <- 0.25
  .lengths <- c(784, 16, 4, 10)
  truth <- matrix(c(0,0,0,0,0,1,0,0,0,0), ncol=1) # 5 (index by 1 means digits[1]=0)
  errs <- list()


  model <- list()
  .nlayers <- length(.lengths)
  model$input <- as.vector(tr.images[[1]])
  model$nodes <- mapply(matrix,
			data=lapply(.lengths,rnorm, sd=.37),
			ncol=1,
			nrow=.lengths)
  model$weights <-  lapply(1:(.nlayers-1),
			   function(k) {
			       matrix(rnorm(.lengths[k+1]*.lengths[k]),
				      nrow=.lengths[k+1],
				      ncol=.lengths[k])})
  model$biases <- mapply(matrix,
			 data=lapply(.lengths[-1], rnorm),
			 ncol=1,
			 nrow=.lengths[-1])

  ## Training
  for(n in 1:1000) {

			      # Feed Forward
      model$nodes[[1]] <- model$input
      model$active[[1]] <- activate(model$nodes[[1]])

      ## loop through the rest
      for(i in 2:.nlayers) {
	  model$nodes[[i]] <- model$weights[[i-1]] %*% model$active[[i-1]] + model$biases[[i-1]]
	  model$active[[i]] <- activate(model$nodes[[i]])
      }

					  # record error of feed forward
					  
      errs[[n]] <- .err

					  # Backprop
      del <- list()
      del[[(.nlayers-1)]] <- (model$active[[.nlayers]] - truth) * sigprime(model$nodes[[.nlayers]])
      ## Loop throught the rest
      for(i in seq((.nlayers-1),2,-1)) {
	  del[[i-1]] <- (t(model$weights[[i]]) %*% del[[i]]) * sigprime(model$nodes[[i]])
      }       

					  # Update                                        
      .new.w <- list()
      for(i in length(model$weights):1) {
	  .new.w[[i]] <- model$weights[[i]] - learningrate * del[[i]] %*% t(model$active[[i]])
      }
      model$weights <- .new.w
      .new.b <- list()
      for(i in length(model$weights):1) {
	  .new.b[[i]] <- model$biases[[i]] - learningrate * del[[i]]
      }
      model$biases <- .new.b
  }
#+END_SRC


** Prediction 


#+BEGIN_SRC R :results both :exports both
which.max(as.vector(model$active[[.nlayers]])) - 1 #
#+END_SRC

#+RESULTS:
: 5


#+BEGIN_SRC R :results both :exports both
model$active[[.nlayers]]
#+END_SRC

#+RESULTS:
| 0.0268742268963425 |
| 0.0276090115141976 |
| 0.0270112449462544 |
| 0.0279671025073374 |
| 0.0275920869288741 |
|  0.972744518193588 |
| 0.0265384659461282 |
| 0.0241555320569751 |
| 0.0270018632725794 |
| 0.0275394257004914 |


** Error

#+BEGIN_SRC R
errs[[length(errs)]] 
#+END_SRC

#+RESULTS:
|  0.0268742268963425 |
|  0.0276090115141976 |
|  0.0270112449462544 |
|  0.0279671025073374 |
|  0.0275920869288741 |
| -0.0272554818064124 |
|  0.0265384659461282 |
|  0.0241555320569751 |
|  0.0270018632725794 |
|  0.0275394257004914 |


#+BEGIN_SRC R :results both :exports both
sum((model$active[[3]] - truth)^2)
#+END_SRC

#+RESULTS:
: 1.15204402985591


* From toy to tool
** model
#+BEGIN_SRC R
ann <- function(node_lengths,
                dlist, l,
                tr.idx, ts.idx,
                learningrate) {
    
    tr.d <- dlist[tr.idx]
    tr.l <- l[tr.idx]
    ts.d <-  dlist[ts.idx]
    ts.l <-  l[ts.idx] 

    model <- new.env()
    model$lengths <- node_lengths
    lengths <- model$lengths
    model$nlayers <- length(model$lengths)
    nlayers <- model$nlayers

    normalize <- function(x){return(x/sum(x))}
    activate <- function(node)
        return(matrix(1/(1+exp(-node))))
    sigprime <- function(node)
        return(matrix(activate(node)*(1 - activate(node))))
    
    model$debug <- TRUE
    model$errs <- list()
    model$biases <- mapply(matrix,
                           data=lapply(lengths[-1], rnorm),
                           ncol=1,
                           nrow=lengths[-1])
    model$weights <- lapply(1:(nlayers-1),
                            function(k) {
                                matrix(rnorm(lengths[k+1]*lengths[k]),
                                       nrow=lengths[k+1],
                                       ncol=lengths[k])})

    ## semi-Pure function: references but does not modify its parent env
    model$predict <- function(input) {
        active <- list()
        nodes <- list()

        nodes[[1]] <- as.vector(input)
        active[[1]] <- activate(nodes[[1]])

        for(i in 2:nlayers) {
            nodes[[i]] <- (model$weights[[i-1]] %*% active[[i-1]])#  + biases[[i-1]])
            active[[i]] <- activate(nodes[[i]])
        }
                
        which.max(as.vector(active[[nlayers]]))-1
    }
        
    train <- function(input, label) {
        truth <- rep(0,10)
        truth[label+1] <- 1

        active <- list()
        nodes <- list()

        nodes[[1]] <- as.vector(input)
        active[[1]] <- activate(nodes[[1]])

        for(i in 2:nlayers) {
            nodes[[i]] <- (weights[[i-1]] %*% active[[i-1]]) #  + biases[[i-1]]
            active[[i]] <- activate(nodes[[i]])
        }
        
        del <- list()
        del[[(nlayers - 1)]] <-  (active[[nlayers]] - truth)* sigprime(nodes[[nlayers]])
        ## n-1, n-2, .. 3, 2
        for(i in seq((nlayers-1), 2, -1)) { 
            del[[i-1]] <- (t(weights[[i]]) %*% del[[i]]) * sigprime(nodes[[i]])
        }
                    
        model$weights <<- lapply(1:length(weights),
                           function(i)
                               return(weights[[i]] - learningrate * del[[i]] %*% t(active[[i]])))

        ## model$biases <<- lapply(1:length(biases),
        ##                   function(i)
        ##                       return(biases[[i]] - learningrate * del[[i]]))
            
        
    }

    
    test <- function(inputs, labels) {        
        preds <- lapply(inputs,model$predict)
        preds==labels
    }

    ## Impure functions
    environment(train) <- model ## MODIFIES ENV
    environment(model$predict) <- model ## Does not modify env   
    
    ## Do initialization
    model$trained <- mapply(train, tr.d, tr.l)
    model$tested <- test(ts.d, ts.l)

    return(model)
}
#+END_SRC

** Testing
#+BEGIN_SRC R
models <- lapply(seq(0.5,1,0.05),
       ann,
       node_lengths=c(784,16,4,10),
       dlist=indata.images,
       l=indata.labels,
       tr.idx=-ts.idx,
       ts.idx=ts.idx)

lapply(models, function(model) length(which(model$tested))) 
#+END_SRC

* Junk that might be useful
#+BEGIN_SRC R
  # ~~~ Junk that might be useful

  ## # save node vectors without activation for backprop
  ## .nodes <- lapply(1:(.nlayers-1),
  ##                   function(k) {
  ##                       model$weights[[k]]%*%model$nodes[[k]]
  ##                       + model$biases[[k]]
  ##                   })

  ## yhat <- activate(model$nodes[[.nlayers]])

  ## ## Backprop
  ## err <- yhat - truth
  ## err

  ## # For every activated output node in err vector, apply element-wise
  ## # multiplication to derivative of activation function of output
  ## # node. This is the "gradient" at the output layer.
  ## nabla <- function(err, layer){
  ##     return(matrix(err*sigprime(.nodes[[layer-1]])))
  ## }

  ## dely <- nabla(err=err,3)
  ## dely


  ## delcdelw2 <- model$nodes[[3]]%*%dely
  ## model$weights[[2]] <- model$weights[[2]] + t(matrix(learningrate*delcdelw2))
  ## model$biases[[2]] <- model$biases[[2]] + learningrate*dely


  ## delw2 <- nabla(err=t(model$weights[[2]])%*%dely, 2)
  ## delw2

  ## delcdelw1 <- model$nodes[[1]]%*%delw2
  ## model$weights[[1]] <- model$weights[[1]] + t(matrix(learningrate*delcdelw1))
  ## model$biases[[1]] <- model$biases[[1]] + learningrate*delw2


  ## model$biases[[2]] <- model$biases[[2]] + delw2 

  ## ???
  ## model$nodes[-1] <- lapply(1:(.nlayers-1),
  ##                           function(k) {
  ##                               activate(model$weights[[k]]%*%model$nodes[[k]]
  ##                                        + model$biases[[k]])
  ##                           })
#+END_SRC

** what does not remain

#+BEGIN_EXAMPLE R
model.gen.annc <- function(layers,
                           training.data,
                           training.labels,
			   debug=TRUE) {    
    .lengths <- layers    
    .nlayers <- length(.lengths)

    if(debug) print(paste("Number of Layers:", n.layers))
    if(debug) print(paste("Layer Lengths:",    toString(.lengths)))
    
    model <- train(training.data, training.labels)
    class(model) <- "model.ann.classifier"
    return(model) 
}


train <- function(data, labels) { # row-wise dataframe and list
    ## lapply(tr.df[,-1],thefollowing) <- do once for each and then average?
    ## split whole db into batches, find average across
    for(n in 1:nrow(data)) {                       
        truth <- rep(0,10)
        truth[labels[n]+1] <- 1
        
        nodes[[1]] <- matrix(data[n,], ncol=1)
        active[[1]] <- activate(nodes[[1]])        
        ## We could definitely get this faster. Each iteration only depends
        ## on the previous layer's active, if we could keep passing that
        ## down the chain
        weights[[1]] %*% active[[1]]

        

        if(debug)
            errs[[n]] <- (model$active[[.nlayers]] - truth)

        del <- list()
        del[[(.nlayers-1)]] <- (.active[[.nlayers]] - truth) * sigprime(.nodes[[.nlayers]])
        ## Loop throught the rest
        for(i in seq((.nlayers-1),2,-1)) {
            del[[i-1]] <- (t(.weights[[i]]) %*% del[[i]]) * sigprime(.nodes[[i]])
        }

        .weights <- lapply(length(.weights):1,
                           function(i)
                               return(.weights[[i]] - learningrate * del[[i]] %*% t(.active[[i]])))
        
        .biases <- lapply(length(.weights):1, 
                          function(i)
                              return(.biases[[i]] - learningrate * del[[i]]))                
    }
    
    trained <- list()
    trained$nodes <- .nodes
    trained$weights <- .weights
    trained$biases <- .biases

    trained$nlayers <- n.layers
    trained$lengths <- .lengths
    
    return(trained)
}

#+END_EXAMPLE

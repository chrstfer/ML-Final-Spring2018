# -*- org-confirm-babel-evaluate: nil; -*-
#+AUTHOR: Matthew Sears
#+TITLE: Artificial Neural Nets 
#+HTML_HEAD: <link href="http://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
#+PROPERTY: header-args :session ANNsb


* Setup
** R Setup  
*** Libraries and Functions
  #+BEGIN_SRC R :results none :export source
    library(sigmoid)
    getLabels <- function(lbldb, nget=0) {
	magic  <- readBin(lbldb, what="integer", n=1, endian="big", size=4)
	if(magic != 2049)
	    return(NULL)
	n.lbls    <- readBin(lbldb, what="integer", n=1,    endian="big", size=4)
	if(nget==0)
	    nget=n.lbls

	labels <- readBin(lbldb, what="integer", n=nget, endian="big",  size=1)

	close(lbldb)
	return(labels)
    }

    getImages <- function(imgdb, nget=0, progress=FALSE) {
	magic  <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	## if(magic != 2049)
	##     return(NULL)

	n.imgs <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	if(nget==0)
	    nget <- n.imgs # trunc(sqrt(n.imgs))

	n.rows <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	n.cols <- readBin(imgdb, what="integer", n=1, endian="big", size=4)

	print(gettextf("Getting %d %dx%d Images", nget, n.rows, n.cols))

	images <- c()
	for(i in 1:nget) {
	    .img   <- matrix(readBin(imgdb, what="integer", n=n.rows*n.cols, endian="big", size=1), nrow=n.rows, ncol=n.cols)
	    images <-  c(images, list(.img))
	    if(progress && i %% trunc(sqrt(nget)) == 0) 
		print(gettextf("%2.2f%%", round((100*i)/nget, digits=2)))
	}
	close(imgdb)
	return(images)
    }
  #+END_SRC
** Data Setup
*** Data (import)
#+BEGIN_SRC R :results output graphics :file imgs/setup/ex1.png
  ## Works
  tr.labels <- as.vector(getLabels(gzfile("datasets/training/labels", "rb"), nget=256))
  tr.images <- getImages(gzfile("datasets/training/images", # data's filename
				"rb"), # read it as binary
			 ## Get 256 of the entries
			 nget=256, progress=TRUE)
  tr.im.matrix <- do.call("rbind", # create rows out of the input data
			  lapply(tr.images, as.vector)) # transform each image
							# matrix into a vector

  tr.df <- cbind(as.factor(tr.labels), tr.im.matrix) # now create a data frame

  oldpar <- par(mar=rep(0,4))
  image(tr.images[[8]], useRaster=TRUE, col=seq(2^8)) 
  par(oldpar)
#+END_SRC

#+RESULTS:
[[file:imgs/setup/ex1.png]]

- Label Frequency ::
#+BEGIN_SRC R :results table drawer :colnames yes :exports results
table(Labels=tr.df[,ncol(tr.df)])
#+END_SRC

#+RESULTS:
:RESULTS:
| Labels | Freq |
|--------+------|
|      0 |  256 |
:END:

*** Data (Links)
  |---------------------+----------+-------------------------------------------------------------|
  | ID                  | size (b) | Link                                                        |
  |---------------------+----------+-------------------------------------------------------------|
  | training set images |  9912422 | http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz |
  | training set labels |    28881 | http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz |
  | test set images     |  1648877 | http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  |
  | test set labels     |     4542 | http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz  |
  |---------------------+----------+-------------------------------------------------------------|



* Simple
  
#+BEGIN_SRC R :exports both :results output
  #rm(list=ls())
  set.seed(1)

  activate <- function(node)
      return(matrix(1/(1+exp(-node))))
  sigprime <- function(node)
      return(matrix(activate(node)*(1 - activate(node))))

  truth <- 1
  learningrate <- 0.25
  errs <- c()
  .lengths <- c(2,3,3,1) # I,H,O


  model <- list()
  .nlayers <- length(.lengths)
  model$input <- matrix(c(1,0))
  model$nodes <- mapply(matrix,
			data=lapply(.lengths,rnorm, sd=.37),
			ncol=1,
			nrow=.lengths)
  model$weights <-  lapply(1:(.nlayers-1),
			   function(k) {
			       matrix(rnorm(.lengths[k+1]*.lengths[k]),
				      nrow=.lengths[k+1],
				      ncol=.lengths[k])})
  model$biases <- mapply(matrix,
			 data=lapply(.lengths[-1], rnorm),
			 ncol=1,
			 nrow=.lengths[-1])

  n <- 0
  for(n in 1:250) {
    
			      # Feed Forward
      model$nodes[[1]] <- model$input
      model$active[[1]] <- activate(model$nodes[[1]])

      ## loop through the rest
      for(i in 2:.nlayers) {
	  model$nodes[[i]] <- model$weights[[i-1]] %*% model$active[[i-1]] + model$biases[[i-1]]
	  model$active[[i]] <- activate(model$nodes[[i]])
      }

					  # record error of feed forward
      errs[n] <- model$active[[.nlayers]] - truth

    
					  # Backprop
      del <- list()
      del[[(.nlayers-1)]] <- (model$active[[.nlayers]] - truth) * sigprime(model$nodes[[.nlayers]])
      ## Loop throught the rest
      for(i in seq((.nlayers-1),2,-1)) {
	  print(i)
	  del[[i-1]] <- (t(model$weights[[i]]) %*% del[[i]]) * sigprime(model$nodes[[i]])
      }       

    
					  # Update                                        
      .new.w <- list()
      for(i in length(model$weights):1) {
	  .new.w[[i]] <- model$weights[[i]] - learningrate * del[[i]] %*% t(model$active[[i]])
      }
      model$weights <- .new.w
      .new.b <- list()
      for(i in length(model$weights):1) {
	  .new.b[[i]] <- model$biases[[i]] - learningrate * del[[i]]
      }
      model$biases <- .new.b
  }

    
#+END_SRC

#+RESULTS:

** Results
#+BEGIN_SRC R :results graphics :exports output :file imgs/tests/basic-errplot.png
plot(abs(errs))
#+END_SRC

#+RESULTS:
[[file:imgs/tests/basic-errplot.png]]

** Notes
   - Why separate active vs nodes? why not `model$nodes <- activate(the multiplication)`?
   - 


* MNIST?

** Training
Using the first training example.
#+BEGIN_SRC R :results none
  set.seed(1)

  ## Keeping everything else the same as toy example above, except for
  ## this stuff right here

  normalize <- function(x)
      return(x/sum(x))
  activate <- function(node)
      return(matrix(1/(1+exp(-node))))
  sigprime <- function(node)
      return(matrix(activate(node)*(1 - activate(node))))

  learningrate <- 0.25
  .lengths <- c(784, 16, 4, 10)
  truth <- matrix(c(0,0,0,0,0,1,0,0,0,0), ncol=1) # 5 (index by 1 means digits[1]=0)
  errs <- list()


  model <- list()
  .nlayers <- length(.lengths)
  model$input <- as.vector(tr.images[[1]])
  model$nodes <- mapply(matrix,
			data=lapply(.lengths,rnorm, sd=.37),
			ncol=1,
			nrow=.lengths)
  model$weights <-  lapply(1:(.nlayers-1),
			   function(k) {
			       matrix(rnorm(.lengths[k+1]*.lengths[k]),
				      nrow=.lengths[k+1],
				      ncol=.lengths[k])})
  model$biases <- mapply(matrix,
			 data=lapply(.lengths[-1], rnorm),
			 ncol=1,
			 nrow=.lengths[-1])

  ## Training
  for(n in 1:1000) {

			      # Feed Forward
      model$nodes[[1]] <- model$input
      model$active[[1]] <- activate(model$nodes[[1]])

      ## loop through the rest
      for(i in 2:.nlayers) {
	  model$nodes[[i]] <- model$weights[[i-1]] %*% model$active[[i-1]] + model$biases[[i-1]]
	  model$active[[i]] <- activate(model$nodes[[i]])
      }

					  # record error of feed forward
      .err <- (model$active[[.nlayers]] - truth)
      errs[[n]] <- .err

					  # Backprop
      del <- list()
      del[[(.nlayers-1)]] <- (model$active[[.nlayers]] - truth) * sigprime(model$nodes[[.nlayers]])
      ## Loop throught the rest
      for(i in seq((.nlayers-1),2,-1)) {
	  del[[i-1]] <- (t(model$weights[[i]]) %*% del[[i]]) * sigprime(model$nodes[[i]])
      }       

					  # Update                                        
      .new.w <- list()
      for(i in length(model$weights):1) {
	  .new.w[[i]] <- model$weights[[i]] - learningrate * del[[i]] %*% t(model$active[[i]])
      }
      model$weights <- .new.w
      .new.b <- list()
      for(i in length(model$weights):1) {
	  .new.b[[i]] <- model$biases[[i]] - learningrate * del[[i]]
      }
      model$biases <- .new.b
  }
#+END_SRC


** Prediction 


#+BEGIN_SRC R :results both :exports both
which.max(as.vector(model$active[[.nlayers]])) - 1 #
#+END_SRC

#+RESULTS:
: 5


#+BEGIN_SRC R :results both :exports both
model$active[[.nlayers]]
#+END_SRC

#+RESULTS:
| 0.0268742268963425 |
| 0.0276090115141976 |
| 0.0270112449462544 |
| 0.0279671025073374 |
| 0.0275920869288741 |
|  0.972744518193588 |
| 0.0265384659461282 |
| 0.0241555320569751 |
| 0.0270018632725794 |
| 0.0275394257004914 |


** Error

#+BEGIN_SRC R
errs[[length(errs)]] 
#+END_SRC

#+RESULTS:
|  0.0268742268963425 |
|  0.0276090115141976 |
|  0.0270112449462544 |
|  0.0279671025073374 |
|  0.0275920869288741 |
| -0.0272554818064124 |
|  0.0265384659461282 |
|  0.0241555320569751 |
|  0.0270018632725794 |
|  0.0275394257004914 |


#+BEGIN_SRC R :results both :exports both
sum((model$active[[3]] - truth)^2)
#+END_SRC

#+RESULTS:
: 1.15204402985591


* Junk that might be useful
#+BEGIN_SRC R
  # ~~~ Junk that might be useful

  ## # save node vectors without activation for backprop
  ## .nodes <- lapply(1:(.nlayers-1),
  ##                   function(k) {
  ##                       model$weights[[k]]%*%model$nodes[[k]]
  ##                       + model$biases[[k]]
  ##                   })

  ## yhat <- activate(model$nodes[[.nlayers]])

  ## ## Backprop
  ## err <- yhat - truth
  ## err

  ## # For every activated output node in err vector, apply element-wise
  ## # multiplication to derivative of activation function of output
  ## # node. This is the "gradient" at the output layer.
  ## nabla <- function(err, layer){
  ##     return(matrix(err*sigprime(.nodes[[layer-1]])))
  ## }

  ## dely <- nabla(err=err,3)
  ## dely


  ## delcdelw2 <- model$nodes[[3]]%*%dely
  ## model$weights[[2]] <- model$weights[[2]] + t(matrix(learningrate*delcdelw2))
  ## model$biases[[2]] <- model$biases[[2]] + learningrate*dely


  ## delw2 <- nabla(err=t(model$weights[[2]])%*%dely, 2)
  ## delw2

  ## delcdelw1 <- model$nodes[[1]]%*%delw2
  ## model$weights[[1]] <- model$weights[[1]] + t(matrix(learningrate*delcdelw1))
  ## model$biases[[1]] <- model$biases[[1]] + learningrate*delw2


  ## model$biases[[2]] <- model$biases[[2]] + delw2 

  ## ???
  ## model$nodes[-1] <- lapply(1:(.nlayers-1),
  ##                           function(k) {
  ##                               activate(model$weights[[k]]%*%model$nodes[[k]]
  ##                                        + model$biases[[k]])
  ##                           })
#+END_SRC


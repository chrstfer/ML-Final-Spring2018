# -*- org-confirm-babel-evaluate: nil; -*-
#+AUTHOR: Chris Carrigan Brolly
#+TITLE: Artificial Neural Nets 
#+HTML_HEAD: <link href="http://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
#+PROPERTY: header-args :session ANNimpl

* Setup
** R Setup  
*** Libraries and Functions
  #+BEGIN_SRC R :results none :export source
    library(sigmoid)
    getLabels <- function(lbldb, nget=0) {
	magic  <- readBin(lbldb, what="integer", n=1, endian="big", size=4)
	if(magic != 2049)
	    return(NULL)
	n.lbls    <- readBin(lbldb, what="integer", n=1,    endian="big", size=4)
	if(nget==0)
	    nget=n.lbls

	labels <- readBin(lbldb, what="integer", n=nget, endian="big",  size=1)

	close(lbldb)
	return(labels)
    }

    getImages <- function(imgdb, nget=0, progress=FALSE) {
	magic  <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	## if(magic != 2049)
	##     return(NULL)

	n.imgs <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	if(nget==0)
	    nget <- n.imgs # trunc(sqrt(n.imgs))

	n.rows <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	n.cols <- readBin(imgdb, what="integer", n=1, endian="big", size=4)

	print(gettextf("Getting %d %dx%d Images", nget, n.rows, n.cols))

	images <- c()
	for(i in 1:nget) {
	    .img   <- matrix(readBin(imgdb, what="integer", n=n.rows*n.cols, endian="big", size=1), nrow=n.rows, ncol=n.cols)
	    images <-  c(images, list(.img))
	    if(progress && i %% trunc(sqrt(nget)) == 0) 
		print(gettextf("%2.2f%%", round((100*i)/nget, digits=2)))
	}
	close(imgdb)
	return(images)
    }
  #+END_SRC
** Data Setup
*** Data (import)
#+BEGIN_SRC R :results output graphics :file imgs/setup/ex1.png
  ## Works
  trnum <- 2048
  tsnum <- 512
  dsetnum <- trnum+tsnum
  in.labels <- as.vector(getLabels(gzfile("datasets/training/labels", "rb"), nget=dsetnum))
  in.images <- getImages(gzfile("datasets/training/images", # data's filename
				"rb"), # read it as binary
			 ## Get 256 of the entries
			 nget=dsetnum, progress=TRUE)

  ts.idx <- sample(dsetnum, 0.15*dsetnum)

  in.df <- cbind(Labels=in.labels, Img=do.call("rbind", lapply(in.images, as.vector)))

  oldpar <- par(mar=rep(0,4))
  image(tr.images[[8]], useRaster=TRUE, col=seq(2^8)) 
  par(oldpar)
#+END_SRC

#+RESULTS:
[[file:imgs/setup/ex1.png]]

- Label Frequency ::
#+BEGIN_SRC R :results table drawer :colnames yes :exports results
table(Labels=tr.labels)
#+END_SRC

#+RESULTS:
:RESULTS:
| Labels | Freq |
|--------+------|
|      0 |  100 |
|      1 |  120 |
|      2 |  101 |
|      3 |   95 |
|      4 |  108 |
|      5 |   92 |
|      6 |   95 |
|      7 |  122 |
|      8 |   90 |
|      9 |  101 |
:END:

*** Data (Links)
  |---------------------+----------+-------------------------------------------------------------|
  | ID                  | size (b) | Link                                                        |
  |---------------------+----------+-------------------------------------------------------------|
  | training set images |  9912422 | http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz |
  | training set labels |    28881 | http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz |
  | test set images     |  1648877 | http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  |
  | test set labels     |     4542 | http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz  |
  |---------------------+----------+-------------------------------------------------------------|

* Implementation: Multilayer Perceptron with Backprop
- Features ::
  - builds arbitrarily layered ANN model
  - has weights (rnorm), biases(rnorm), and layer nodes
  - has member function for layer extraction
    - returns class "layer" with 3 member lists for the given layers data
- TODO ::
  - predict function :: feedforward (recursive chain of feedforwardstep?)
  - train function ::   backprop (step then recursive chain?)
  - summary functions :: layer and model
  - print functions :: layer, model
  - plot functions :: model, inputs, outputs (eg plot the input rows of tr.df,
                      and output of predict
** Model
#+BEGIN_SRC R
  ann <- function(node_lengths,
		  dlist, lbls,
		  tr.idx=0, ts.idx=0,
		  learningrate) {
      if(tr.idx==0 || ts.idx==0) {
	  tr.d <- dlist
	  tr.l <- lbls
	  do.test <- FALSE
      } else {        
	  tr.d <- dlist[tr.idx]
	  tr.l <- lbls[tr.idx]
	  ts.d <-  dlist[ts.idx]
	  ts.l <-  lbls[ts.idx]
	  do.test <- TRUE
      }

      model <- new.env()
      model$lengths <- node_lengths
      lengths <- model$lengths
      model$nlayers <- length(model$lengths)
      nlayers <- model$nlayers

      normalize <- function(x){return(x/sum(x))}
      activate <- function(node)
	  return(matrix(1/(1+exp(-node))))
      sigprime <- function(node)
	  return(matrix(activate(node)*(1 - activate(node))))

      model$debug <- TRUE
      model$errs <- list()
      model$biases <- mapply(matrix,
			     data=lapply(lengths[-1], rnorm),
			     ncol=1,
			     nrow=lengths[-1])
      model$weights <- lapply(1:(nlayers-1),
			      function(k) {
				  matrix(rnorm(lengths[k+1]*lengths[k]),
					 nrow=lengths[k+1],
					 ncol=lengths[k])})

      ## semi-Pure function: references but does not modify its parent env
      model$predict <- function(input) {
	  active <- list()
	  nodes <- list()

	  nodes[[1]] <- as.vector(input)
	  active[[1]] <- activate(nodes[[1]])

	  for(i in 2:nlayers) {
	      nodes[[i]] <- (model$weights[[i-1]] %*% active[[i-1]]  + biases[[i-1]])
	      active[[i]] <- activate(nodes[[i]])
	  }

	  which.max(as.vector(active[[nlayers]]))-1
      }

      train <- function(input, label) {
	  truth <- rep(0,10)
	  truth[label+1] <- 1

	  active <- list()
	  nodes <- list()

	  nodes[[1]] <- as.vector(input)
	  active[[1]] <- activate(nodes[[1]])

	  for(i in 2:nlayers) {
	      nodes[[i]] <- (weights[[i-1]] %*% active[[i-1]]  + biases[[i-1]])
	      active[[i]] <- activate(nodes[[i]])
	  }

	  del <- list()
	  del[[(nlayers - 1)]] <-  (active[[nlayers]] - truth)* sigprime(nodes[[nlayers]])
	  ## n-1, n-2, .. 3, 2
	  for(i in seq((nlayers-1), 2, -1)) { 
	      del[[i-1]] <- (t(weights[[i]]) %*% del[[i]]) * sigprime(nodes[[i]])
	  }

	  model$weights <<- lapply(1:length(weights),
				   function(i)
				       return(weights[[i]] - learningrate * del[[i]] %*% t(active[[i]])))

	  model$biases <<- lapply(1:length(biases),
				  function(i)
				      return(biases[[i]] - learningrate * del[[i]]))                    
      }


      test <- function(inputs, labels) {        
	  preds <- lapply(inputs,model$predict)
	  preds==labels
      }

      ## Impure functions
      environment(train) <- model ## MODIFIES ENV
      environment(model$predict) <- model ## Does not modify env   

      ## Do initialization
      model$trained <- mapply(train, tr.d, tr.l)
      if(do.test) {
	  model$tested <- test(ts.d, ts.l)
      }

      return(model)
  } 
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R

#+END_SRC

#+RESULTS:

* Analysis
#+BEGIN_SRC R :results value drawer
  set.seed(420)

  ## separate dset into groups on 128

  tr.im <- in.images[-ts.idx]
  tr.lb <- in.labels[-ts.idx]
  ts.im <- in.images[ts.idx]
  ts.lb <- in.labels[ts.idx]


  .size <- 128
  .num  <- trnum/mbsize


  for(mb in 1:.num) {
  
  }


  lrs<-seq(-0.1, 1, 0.05)
  models <- lapply(lrs,
	 ann,
	 node_lengths=c(784, 16, 4,4,10),
	 dlist=,
	 l=in.labels)


  results <- lapply(models, function(model) length(which(model$tested)))
  paste("Accuracy%: ",max(sort(unlist(results),decreasing=TRUE))/64)
#+END_SRC

#+RESULTS:
:RESULTS:
Accuracy%:  0.296875
:END:

* Conclusion
  We have somehow trained a network to be worse than a coin. Great.

  - Conclusions :: Going from unrolled hardcoded toy example that trains great
                   for one image to an interface that can actually be trained on
                   the dataset is much harder than a 1 night task.

** Comparison

#+BEGIN_SRC R 
  ## lets try with pca
  pcs <- prcomp(in.df[,-1])
  pca.im <- pcs$x[,1:8] #split(...,row(pcs$x[,1:8]))
  pca.tr <- pca.im[-ts.idx,]
  pca.ts <- pca.im[ts.idx,]
  linm <- lm(in.labels[-ts.idx]~pca.tr)
  pred <- predict(linm, newdata=data.frame(pca.ts), )


#+END_SRC

* Sources
** Biblio
   These I read in the process of completing this project. In places where
   specific citations could be made, I have places them and linked here. 

- https://journal.r-project.org/archive/2010-1/RJournal_2010-1_Guenther+Fritsch.pdf
- https://en.wikipedia.org/wiki/Perceptron
- https://cran.r-project.org/web/packages/sigmoid/sigmoid.pdf
*** backprop
    https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py

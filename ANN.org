# -*- org-confirm-babel-evaluate: nil; -*-
#+AUTHOR: Chris Carrigan Brolly
#+TITLE: Artificial Neural Nets 
#+HTML_HEAD: <link href="http://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
#+PROPERTY: header-args :session ANNimpl

* Introduction
Machine Learning Final Project -- Spring 2018 at Wentworth Institute of Technology

** MNIST

We will use the MNIST hand-written digits dataset, as it's the canonical dataset
for state-of-the-art machine learning algorithms simply because of how clean and
well-crafted it is.

Here's an example of the matrix representation of a 28x28 pixel image of a
hand-written digit:

file:https://www.tensorflow.org/versions/r1.1/images/MNIST-Matrix.png


** Procedure
We convert these matrices into column vectors in =R=, where each row represents
an individual pixel. This turns into our input layer for our neural network
where each input node, or sigmoid neuron, is a pixel.

file:https://ml4a.github.io/images/figures/mnist_2layers.png

The hidden layer is one of the main reasons why neural networks are
successful. It essentially provides an additional layer of abstraction (no pun
intended).

There are 10 digits in total, which means we have 10 classes for our model, and
subsequently 10 output neurons in our output layer.

_Activation Function:_

This is what allows us to cook with gas -- an activation function involves the
introduction of a non-linearity. It is often viewed analagous to the rate of
action potential firing in the brain, and basically turns our previously linear
classifiers into more flexible perceptrons called neurons.

We used the sigmoid function:
\[ 
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

where, for example with a single hidden neuron $z$,

$$z = {\bf w} \cdot x + b$$

The first part of that sum is the dot product of the total input going
into the neuron, i.e., the total/net weighted inputs. The second part
is the bias associated with the neuron, which is basically analogous
to how hard/easy it is for that neuron to fire.

Useful properties of the sigmoid function:

- "Squashes" any input into the range $[0,1]$ 
- Easy to use when training our model with backpropagation
  + $\sigma^\prime(z) = \sigma(z) (1-\sigma(z))$


\[
\lim_{z \to -\infty} \frac{1}{1 + e^{-z}} = 0 \\
\lim_{z \to \infty}  \frac{1}{1 + e^{-z}} = \frac{1}{2} \\
\lim_{z \to 0}       \frac{1}{1 + e^{-z}} = 1 \\
\]

*** Backpropagation Algorithm

Without backpropagation, the model would never learn. Learning in this
case means having the model tune its weights and biases such that the
error is minimized through the cost function it's given.

**** _Cost Function:_ 

$$C(w,b) = \frac{1}{2} \left| y - \hat{y} \right| ^2$$

Our parameters are weights and biases, and we define our cost function
as something convex to guarantee a global minimum during gradient
descent.

Backpropagation requires multivariable calculus, which I go into
detail in a write-up [[https://matthewsears.github.io/img/main.pdf][here]] in the context of an XOR gate. In short,
backpropagation is a clever way of using the chain rule to update the
weights and biases.

For our algorithm we only need to focus on these equations:
file:http://neuralnetworksanddeeplearning.com/images/tikz21.png

**** _Feed-Forward:_

Before backpropogation we run the first feed-forward iteration, which
is just running the network start to finish on the randomly
initialized weights and biases and comparing its junk output to the
desired output.

We focus on layers $l = 2, \ldots, L$ to feed the activated input
forward through the network.

For our output layer, $L = 3$, and our single hidden layer is $l = L -
1 = 2$.

**** _Back-Prop_

The gradient evaluated at the activated output layer, $3$:
$$\delta^3 = \nabla_a C \odot \sigma'(z^3)$$

where $z^3$ is the unactivated node vector in the output layer
***** TODO : talk about $\odot$ and a=sig(z)

Backpropagating the errors into the hidden layer:
$$\delta^2 = ((w^{3})^T \delta^{3}) \odot \sigma'(z^2)$$

where $w^3$ is the weight matrix that influences the output layer.

Our output is then the gradient of the cost function, which we defined
as all the individual tunings we apply to the weights and biases after
every backprop step.

$$\nabla C = \alpha \left< \frac{\partial C}{\partial w^l} := a^{l-1}
\delta^l, \frac{\partial C}{\partial b^l} := \delta^l \right>$$

for all $l = 2, \ldots, L$ weight/bias matrices, where $\alpha \in
(0,1)$ is the learning rate, or how fast it tries to approach the
minimum with every backprop step.

***** TODO : high-level explanatory comments

**** _Update_

We then proceed to update the weights and biases based on the
gradient, subtracting from the previous iteration's values since it's
descent.

\[
w^3 -= \alpha \left(\delta^3(a^2)^T \right) \\
w^2 -= \alpha \left(\delta^2(a^1)^T \right) \\
b^3 -= \alpha \left(\delta^3 \right) \\
b^2 -= \alpha \left(\delta^2 \right) \\
\]

** Reference
http://neuralnetworksanddeeplearning.com/
* Code
** Setup
*** R Setup  
**** Libraries and Functions
   #+BEGIN_SRC R :results none :export source
     library(sigmoid)
     getLabels <- function(lbldb, nget=0) {
	 magic  <- readBin(lbldb, what="integer", n=1, endian="big", size=4)
	 if(magic != 2049)
	     return(NULL)
	 n.lbls    <- readBin(lbldb, what="integer", n=1,    endian="big", size=4)
	 if(nget==0)
	     nget=n.lbls

	 labels <- readBin(lbldb, what="integer", n=nget, endian="big",  size=1)

	 close(lbldb)
	 return(labels)
     }

     getImages <- function(imgdb, nget=0, progress=FALSE) {
	 magic  <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	 ## if(magic != 2049)
	 ##     return(NULL)

	 n.imgs <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	 if(nget==0)
	     nget <- n.imgs # trunc(sqrt(n.imgs))

	 n.rows <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	 n.cols <- readBin(imgdb, what="integer", n=1, endian="big", size=4)

	 print(gettextf("Getting %d %dx%d Images", nget, n.rows, n.cols))

	 images <- c()
	 for(i in 1:nget) {
	     .img   <- matrix(readBin(imgdb, what="integer", n=n.rows*n.cols, endian="big", size=1), nrow=n.rows, ncol=n.cols)
	     images <-  c(images, list(.img))
	     if(progress && i %% trunc(sqrt(nget)) == 0) 
		 print(gettextf("%2.2f%%", round((100*i)/nget, digits=2)))
	 }
	 close(imgdb)
	 return(images)
     }
   #+END_SRC
*** Data Setup
**** Data (import)
 #+BEGIN_SRC R :results output graphics :file imgs/setup/ex1.png
   ## Works
   trnum <- 2048
   tsnum <- 512
   dsetnum <- trnum+tsnum
   in.labels <- as.vector(getLabels(gzfile("datasets/training/labels", "rb"), nget=dsetnum))
   in.images <- getImages(gzfile("datasets/training/images", # data's filename
				 "rb"), # read it as binary
			  ## Get 256 of the entries
			  nget=dsetnum, progress=TRUE)

   fives <- which(in.labels==5)
   ts.fives.idx <- sample(fives, 0.15*length(fives))
   ts.fives <- in.images[ts.fives.idx]
   ts.fives.l <- in.labels[ts.fives.idx]
   tr.fives <- in.images[-ts.fives.idx]
   tr.fives.l <- in.labels[-ts.fives.idx]

   ts.idx <- sample(dsetnum, 0.15*dsetnum)

   in.df <- cbind(Labels=in.labels, Img=do.call("rbind", lapply(in.images, as.vector)))

   oldpar <- par(mar=rep(0,4))
   image(tr.images[[8]], useRaster=TRUE, col=seq(2^8)) 
   par(oldpar)
 #+END_SRC

 #+RESULTS:
 [[file:imgs/setup/ex1.png]]

 - Label Frequency ::
 #+BEGIN_SRC R :results table drawer :colnames yes :exports results
 table(Labels=tr.labels)
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 | Labels | Freq |
 |--------+------|
 |      0 |  100 |
 |      1 |  120 |
 |      2 |  101 |
 |      3 |   95 |
 |      4 |  108 |
 |      5 |   92 |
 |      6 |   95 |
 |      7 |  122 |
 |      8 |   90 |
 |      9 |  101 |
 :END:

**** Data (Links)
   |---------------------+----------+-------------------------------------------------------------|
   | ID                  | size (b) | Link                                                        |
   |---------------------+----------+-------------------------------------------------------------|
   | training set images |  9912422 | http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz |
   | training set labels |    28881 | http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz |
   | test set images     |  1648877 | http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  |
   | test set labels     |     4542 | http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz  |
   |---------------------+----------+-------------------------------------------------------------|

** Implementation: Multilayer Perceptron with Backprop
 - Features ::
   - builds arbitrarily layered ANN model
   - has weights (rnorm), biases(rnorm), and layer nodes
   - has member function for layer extraction
     - returns class "layer" with 3 member lists for the given layers data
 - TODO ::
   - predict function :: feedforward (recursive chain of feedforwardstep?)
   - train function ::   backprop (step then recursive chain?)
   - summary functions :: layer and model
   - print functions :: layer, model
   - plot functions :: model, inputs, outputs (eg plot the input rows of tr.df,
                       and output of predict
*** Model
 #+BEGIN_SRC R
   ann <- function(node_lengths,
		   dlist, lbls,
		   tr.idx=0, ts.idx=0,
		   learningrate) {

       if(tr.idx==0 || ts.idx==0) {
	   tr.d <- lapply(dlist, as.vector)
	   tr.l <- lbls
	   do.test <- FALSE
       } else {        
	   tr.d <- dlist[tr.idx]
	   tr.l <- lbls[tr.idx]
	   ts.d <-  dlist[ts.idx]
	   ts.l <-  lbls[ts.idx]
	   do.test <- TRUE
       }

       model <- new.env()
       model$lengths <- node_lengths
       lengths <- model$lengths
       model$nlayers <- length(model$lengths)
       nlayers <- model$nlayers

       ## normalize <- function(x)
       ##     return(x/sum(x))
       activate <- function(node)
	   return(matrix(1/(1+exp(-node))))
       sigprime <- function(node)
	   return(matrix(activate(node)*(1 - activate(node))))

       model$debug <- TRUE
       model$errs <- list()
       model$biases <- mapply(matrix,
			      data=lapply(lengths[-1], rnorm),
			      ncol=1,
			      nrow=lengths[-1])
       model$weights <- lapply(2:nlayers,
			       function(k) {
				   matrix(rnorm(lengths[k]*lengths[k-1]),
					  nrow=lengths[k],
					  ncol=lengths[k-1])})

       ## semi-Pure function: references but does not modify its parent env
       model$predict <- function(input) {
	   active <- list()
	   nodes <- list()

	   nodes[[1]] <- input
	   active[[1]] <- activate(nodes[[1]])

	   for(i in 2:nlayers) {
	       nodes[[i]] <- (model$weights[[i]] %*% active[[i-1]]  + biases[[i-1]])
	       active[[i]] <- activate(nodes[[i]])
	   }
	   which.max(as.vector(active[[nlayers]]))-1
       }

       train <- function(input, label) {
	   truth <- rep(0,10)
	   truth[label+1] <- 1

	   invec <- input

	   active <- list()
	   nodes <- list()

	   nodes[[1]] <- input
	   active[[1]] <- as.matrix(activate(nodes[[1]]))

	   for(i in 2:nlayers) {
	       nodes[[i]] <- (weights[[i]] %*% active[[i-1]])  + biases[[i-1]]
	       active[[i]] <- activate(nodes[[i]])
	   }

	   del <- list()
	   ##print(active[[nlayers]])
	   ##print(weights[[nlayers-1]])
	   del[[nlayers]] <- (active[[nlayers]] - truth) * sigprime(nodes[[nlayers]])
	   ## n-1, n-2, .. 3, 2
	   for(i in seq(nlayers, 2, -1)) {
	       ## print(i)
	       del[[i-1]] <- (t(model$weights[[i]]) %*% del[[i]]) * sigprime(nodes[[i]])
	   }

	   weights <<- lapply(2:length(weights),
			      function(i) {
				  print(del[[i]])
				  print(i)
				  return( weights[[i]] - learningrate * (del[[i]] %*% t(active[[i]])))
			      })
	   biases <<- lapply(1:length(biases),
			     function(i)
				 return(biases[[i]] - learningrate * del[[i]]))                 
       }

       test <- function(inputs, labels) {        
	   preds <- lapply(inputs,model$predict)
	   preds==labels
       }

       ## Impure functions
       environment(train)         <- model         ## MODIFIES ENV
       environment(model$predict) <- model ## Does not modify env   

       ## Do initialization
       model$trained <- mapply(train, tr.d, tr.l)
       if(do.test) {
	   model$tested <- test(ts.d, ts.l)
       }

       return(model)
   } 
 #+END_SRC

 #+RESULTS:

 #+BEGIN_SRC R
 model <- ann(c(784,16,16,1),tr.fives,tr.fives.l,learningrate=0.05)
 #+END_SRC

 #+RESULTS:
 : TRUE

* Analysis
 #+BEGIN_SRC R :results value drawer
   set.seed(420)

   ## separate dset into groups on 128

   tr.im <- in.images[-ts.idx]
   tr.lb <- in.labels[-ts.idx]
   ts.im <- in.images[ts.idx]
   ts.lb <- in.labels[ts.idx]


   .size <- 128
   .num  <- trnum/mbsize


   for(mb in 1:.num) {
  
   }


   lrs<-seq(-0.1, 1, 0.05)
   models <- lapply(lrs,
	  ann,
	  node_lengths=c(784, 16, 4,4,10),
	  dlist=,
	  l=in.labels)


   results <- lapply(models, function(model) length(which(model$tested)))
   paste("Accuracy%: ",max(sort(unlist(results),decreasing=TRUE))/64)
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 Accuracy%:  0.296875
 :END:

* Conclusion
  Difficulty with the language (and the functional paradigm) made it tough to
  implement the features we initially envisioned. 

*** Comparison

 #+BEGIN_SRC R 
   ## lets try with pca
   pcs <- prcomp(in.df[,-1])
   pca.im <- pcs$x[,1:8] #split(...,row(pcs$x[,1:8]))
   pca.tr <- pca.im[-ts.idx,]
   pca.ts <- pca.im[ts.idx,]
   linm <- lm(in.labels[-ts.idx]~pca.tr)
   pred <- predict(linm, newdata=data.frame(pca.ts), )


 #+END_SRC

* Sources
** Biblio
   These I read in the process of completing this project. In places where
   specific citations could be made, I have places them and linked here. 

- https://journal.r-project.org/archive/2010-1/RJournal_2010-1_Guenther+Fritsch.pdf
- https://en.wikipedia.org/wiki/Perceptron
- https://cran.r-project.org/web/packages/sigmoid/sigmoid.pdf
*** backprop
    https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py

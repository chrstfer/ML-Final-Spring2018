# -*- org-confirm-babel-evaluate: nil; -*-
#+AUTHOR: Chris Carrigan Brolly
#+TITLE: Artificial Neural Nets 
#+HTML_HEAD: <link href="http://gongzhitaao.org/orgcss/org.css" rel="stylesheet" type="text/css" />
#+PROPERTY: header-args :session ANNimpl

* Introduction
Machine Learning Final Project -- Spring 2018 at Wentworth Institute of Technology

** TODO 

- Make in-line images show up

  file:http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png

- Fix up $\LaTeX$

** MNIST

We will use the MNIST hand-written digits dataset, as it's the
canonical dataset for state-of-the-art machine learning algorithms
simply because of how clean and well-crafted it is.

Here's an example of the matrix representation of a 28x28 pixel image
of a hand-written digit: 

file:https://www.tensorflow.org/versions/r1.1/images/MNIST-Matrix.png


** Procedure
We convert these matrices into column vectors in =R=, where each row
represents an individual pixel. This turns into our input layer for
our neural network where each input node, or sigmoid neuron, is a
pixel.

file:https://ml4a.github.io/images/figures/mnist_2layers.png

The hidden layer is one of the main reasons why neural networks are
successful. It essentially provides an additional layer of abstraction
(no pun intended).

There are 10 digits in total, which means we have 10 classes for our
model, and subsequently 10 output neurons in our output layer.

_Activation Function:_

This is what allows us to cook with gas -- an activation function
involves the introduction of a non-linearity. It is often viewed
analagous to the rate of action potential firing in the brain, and
basically turns our previously linear classifiers into more flexible
perceptrons called neurons.

We used the sigmoid function:
\[ 
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

where, for example with a single hidden neuron $z$,

$$z = {\bf w} \cdot x + b$$

The first part of that sum is the dot product of the total input going
into the neuron, i.e., the total/net weighted inputs. The second part
is the bias associated with the neuron, which is basically analogous
to how hard/easy it is for that neuron to fire.

**** TODO : big/small and  +/- weights/biases correspond to? 
Useful properties of the sigmoid function:

- "Squashes" any input into the range $[0,1]$ 
- Easy to use when training our model with backpropagation
  + $\sigma^\prime(z) = \sigma(z) (1-\sigma(z))$


\[
\lim_{z \to -\infty} \frac{1}{1 + e^{-z}} = 0 \\
\lim_{z \to \infty}  \frac{1}{1 + e^{-z}} = \frac{1}{2} \\
\lim_{z \to 0}       \frac{1}{1 + e^{-z}} = 1 \\
\]

*** Backpropagation

Without backpropagation, the model would never learn. Learning in this
case means having the model tune its weights and biases such that the
cost function it's given is minimized through gradient descent.

_Cost Function:_

file:http://neuralnetworksanddeeplearning.com/images/tikz21.png



** Reference
http://neuralnetworksanddeeplearning.com/
* Code
** Setup
*** R Setup  
**** Libraries and Functions
   #+BEGIN_SRC R :results none :export source
     library(sigmoid)
     getLabels <- function(lbldb, nget=0) {
	 magic  <- readBin(lbldb, what="integer", n=1, endian="big", size=4)
	 if(magic != 2049)
	     return(NULL)
	 n.lbls    <- readBin(lbldb, what="integer", n=1,    endian="big", size=4)
	 if(nget==0)
	     nget=n.lbls

	 labels <- readBin(lbldb, what="integer", n=nget, endian="big",  size=1)

	 close(lbldb)
	 return(labels)
     }

     getImages <- function(imgdb, nget=0, progress=FALSE) {
	 magic  <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	 ## if(magic != 2049)
	 ##     return(NULL)

	 n.imgs <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	 if(nget==0)
	     nget <- n.imgs # trunc(sqrt(n.imgs))

	 n.rows <- readBin(imgdb, what="integer", n=1, endian="big", size=4)
	 n.cols <- readBin(imgdb, what="integer", n=1, endian="big", size=4)

	 print(gettextf("Getting %d %dx%d Images", nget, n.rows, n.cols))

	 images <- c()
	 for(i in 1:nget) {
	     .img   <- matrix(readBin(imgdb, what="integer", n=n.rows*n.cols, endian="big", size=1), nrow=n.rows, ncol=n.cols)
	     images <-  c(images, list(.img))
	     if(progress && i %% trunc(sqrt(nget)) == 0) 
		 print(gettextf("%2.2f%%", round((100*i)/nget, digits=2)))
	 }
	 close(imgdb)
	 return(images)
     }
   #+END_SRC
*** Data Setup
**** Data (import)
 #+BEGIN_SRC R :results output graphics :file imgs/setup/ex1.png
   ## Works
   trnum <- 2048
   tsnum <- 512
   dsetnum <- trnum+tsnum
   in.labels <- as.vector(getLabels(gzfile("datasets/training/labels", "rb"), nget=dsetnum))
   in.images <- getImages(gzfile("datasets/training/images", # data's filename
				 "rb"), # read it as binary
			  ## Get 256 of the entries
			  nget=dsetnum, progress=TRUE)

   fives <- which(in.labels==5)
   ts.fives.idx <- sample(fives, 0.15*length(fives))
   ts.fives <- in.images[ts.fives.idx]
   ts.fives.l <- in.labels[ts.fives.idx]
   tr.fives <- in.images[-ts.fives.idx]
   tr.fives.l <- in.labels[-ts.fives.idx]

   ts.idx <- sample(dsetnum, 0.15*dsetnum)

   in.df <- cbind(Labels=in.labels, Img=do.call("rbind", lapply(in.images, as.vector)))

   oldpar <- par(mar=rep(0,4))
   image(tr.images[[8]], useRaster=TRUE, col=seq(2^8)) 
   par(oldpar)
 #+END_SRC

 #+RESULTS:
 [[file:imgs/setup/ex1.png]]

 - Label Frequency ::
 #+BEGIN_SRC R :results table drawer :colnames yes :exports results
 table(Labels=tr.labels)
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 | Labels | Freq |
 |--------+------|
 |      0 |  100 |
 |      1 |  120 |
 |      2 |  101 |
 |      3 |   95 |
 |      4 |  108 |
 |      5 |   92 |
 |      6 |   95 |
 |      7 |  122 |
 |      8 |   90 |
 |      9 |  101 |
 :END:

**** Data (Links)
   |---------------------+----------+-------------------------------------------------------------|
   | ID                  | size (b) | Link                                                        |
   |---------------------+----------+-------------------------------------------------------------|
   | training set images |  9912422 | http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz |
   | training set labels |    28881 | http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz |
   | test set images     |  1648877 | http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz  |
   | test set labels     |     4542 | http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz  |
   |---------------------+----------+-------------------------------------------------------------|

** Implementation: Multilayer Perceptron with Backprop
 - Features ::
   - builds arbitrarily layered ANN model
   - has weights (rnorm), biases(rnorm), and layer nodes
   - has member function for layer extraction
     - returns class "layer" with 3 member lists for the given layers data
 - TODO ::
   - predict function :: feedforward (recursive chain of feedforwardstep?)
   - train function ::   backprop (step then recursive chain?)
   - summary functions :: layer and model
   - print functions :: layer, model
   - plot functions :: model, inputs, outputs (eg plot the input rows of tr.df,
                       and output of predict
*** Model
 #+BEGIN_SRC R
   ann <- function(node_lengths,
		   dlist, lbls,
		   tr.idx=0, ts.idx=0,
		   learningrate) {

       if(tr.idx==0 || ts.idx==0) {
	   tr.d <- lapply(dlist,as.vector)
	   tr.l <- lbls
	   do.test <- FALSE
       } else {        
	   tr.d <- dlist[tr.idx]
	   tr.l <- lbls[tr.idx]
	   ts.d <-  dlist[ts.idx]
	   ts.l <-  lbls[ts.idx]
	   do.test <- TRUE
       }

       model <- new.env()
       model$lengths <- node_lengths
       lengths <- model$lengths
       model$nlayers <- length(model$lengths)
       nlayers <- model$nlayers

       normalize <- function(x){return(x/sum(x))}
       activate <- function(node)
	   return(matrix(1/(1+exp(-node))))
       sigprime <- function(node)
	   return(matrix(activate(node)*(1 - activate(node))))

       model$debug <- TRUE
       model$errs <- list()
       model$biases <- mapply(matrix,
			      data=lapply(lengths[-1], rnorm),
			      ncol=1,
			      nrow=lengths[-1])
       model$weights <- lapply(1:(nlayers-1),
			       function(k) {
				   matrix(rnorm(lengths[k+1]*lengths[k]),
					  nrow=lengths[k+1],
					  ncol=lengths[k])})

       ## semi-Pure function: references but does not modify its parent env
       model$predict <- function(input) {
	   active <- list()
	   nodes <- list()

	   nodes[[1]] <- input
	   active[[1]] <- activate(nodes[[1]])

	   for(i in 2:nlayers) {
	       nodes[[i]] <- (model$weights[[i-1]] %*% active[[i-1]]  + biases[[i-1]])
	       active[[i]] <- activate(nodes[[i]])
	   }

	   which.max(as.vector(active[[nlayers]]))-1
       }

       train <- function(input, label) {
	   truth <- rep(0,10)
	   truth[label+1] <- 1

	   invec <- input

	   active <- list()
	   nodes <- list()

	   nodes[[1]] <- input
	   active[[1]] <- as.matrix(activate(nodes[[1]]))

	   for(i in 2:nlayers) {
	       nodes[[i]] <- (weights[[i-1]] %*% active[[i-1]])  + biases[[i-1]]
	       active[[i]] <- activate(nodes[[i]])
	   }

	   del <- list()
	   print(active[[nlayers]])
	   del[[(nlayers - 1)]] <-  (active[[nlayers]] - truth) * sigprime(nodes[[nlayers]])
	   ## n-1, n-2, .. 3, 2
	   for(i in seq((nlayers-1), 2, -1)) {
	       print(i)
	       del[[i-1]] <- (t(model$weights[[i]]) %*% del[[i]])# * sigprime(nodes[[i]])
	   }

	   weights <<- lapply(1:length(weights),
			      function(i)
				  return((weights[[i]] - learningrate * del[[i]]) %*% t(active[[i]])))
	   biases <<- lapply(1:length(biases),
			     function(i)
				 return(biases[[i]] - learningrate * del[[i]]))                    
       }

       test <- function(inputs, labels) {        
	   preds <- lapply(inputs,model$predict)
	   preds==labels
       }

       ## Impure functions
       environment(train)         <- model         ## MODIFIES ENV
       environment(model$predict) <- model ## Does not modify env   

       ## Do initialization
       model$trained <- mapply(train, tr.d, tr.l)
       if(do.test) {
	   model$tested <- test(ts.d, ts.l)
       }

       return(model)
   } 
 #+END_SRC

 #+RESULTS:

 #+BEGIN_SRC R
 model <- ann(c(784,16,16,1),tr.fives,tr.fives.l,learningrate=0.05)
 #+END_SRC

 #+RESULTS:
 : TRUE

* Analysis
 #+BEGIN_SRC R :results value drawer
   set.seed(420)

   ## separate dset into groups on 128

   tr.im <- in.images[-ts.idx]
   tr.lb <- in.labels[-ts.idx]
   ts.im <- in.images[ts.idx]
   ts.lb <- in.labels[ts.idx]


   .size <- 128
   .num  <- trnum/mbsize


   for(mb in 1:.num) {
  
   }


   lrs<-seq(-0.1, 1, 0.05)
   models <- lapply(lrs,
	  ann,
	  node_lengths=c(784, 16, 4,4,10),
	  dlist=,
	  l=in.labels)


   results <- lapply(models, function(model) length(which(model$tested)))
   paste("Accuracy%: ",max(sort(unlist(results),decreasing=TRUE))/64)
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 Accuracy%:  0.296875
 :END:

* Conclusion
  Difficulty with the language (and the functional paradigm) made it tough to
  implement the features we initially envisioned. 

*** Comparison

 #+BEGIN_SRC R 
   ## lets try with pca
   pcs <- prcomp(in.df[,-1])
   pca.im <- pcs$x[,1:8] #split(...,row(pcs$x[,1:8]))
   pca.tr <- pca.im[-ts.idx,]
   pca.ts <- pca.im[ts.idx,]
   linm <- lm(in.labels[-ts.idx]~pca.tr)
   pred <- predict(linm, newdata=data.frame(pca.ts), )


 #+END_SRC

* Sources
** Biblio
   These I read in the process of completing this project. In places where
   specific citations could be made, I have places them and linked here. 

- https://journal.r-project.org/archive/2010-1/RJournal_2010-1_Guenther+Fritsch.pdf
- https://en.wikipedia.org/wiki/Perceptron
- https://cran.r-project.org/web/packages/sigmoid/sigmoid.pdf
*** backprop
    https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py
